{"cells": [{"cell_type": "markdown", "id": "95cf9aa9", "metadata": {}, "source": ["# Les moindres carr\u00e9s via une d\u00e9composition QR \u2615\ufe0f\n", "## I. Introduction\n", "\n", "En *data science*, nous ne faisons pas seulement face \u00e0 des mod\u00e8les math\u00e9matiques que nous souhaitons programmer. Nous devons aussi tenir compte du fait que les nombres que nous manipulons ont une repr\u00e9sentation sur la machine qui les stocke. De mani\u00e8re paradigmatique, consid\u00e9rons la fonction suivante&nbsp;:\n", "\n", "$$f(x)=\\text{ln}\\big(\\text{exp}(x)\\big)=x.$$\n", "\n", "Les deux formulations sont parfaitement indentiques. Observons cela via $\\texttt{numpy}$"]}, {"cell_type": "code", "execution_count": null, "id": "a7ad126a", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "id": "da9e0441", "metadata": {}, "outputs": [], "source": ["x = np.linspace(0, 1000, 500)\n", "y_1 = np.log(np.exp(x))\n", "y_2 = x\n", "\n", "plt.figure(figsize=(12, 8))\n", "\n", "plt.plot(x, y_2, label=r'$x$')\n", "plt.plot(x, y_1, '--', label=r'$ln(exp(x))$')\n", "\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "dc717b0e", "metadata": {}, "source": ["Nous rencontrons une erreur ! Cela vient \u00e9videmment du fait que le calcul de l'exponentielle lorsque $x$ devient trop grand induit un $\\texttt{overflow}$ que le logarithme ne peut plus interpr\u00e9ter. En observant $\\text{ln}\\big(\\text{exp}(x)\\big)=x$, nous avons en quelque sorte utilis\u00e9 une astuce (clairement triviale ici) math\u00e9matique nous permettant d'obtenir notre r\u00e9sultat malgr\u00e9 tout. De mani\u00e8re similaire, nous utilisons souvent en *machine learning* la fonction *softmax*:\n", "\n", "$$\\text{softmax}(x)_j=\\frac{e^{x_j}}{\\sum_i e^{x_i}},$$\n", "\n", "notamment comme fonction de lien en *deep learning* ou en *r\u00e9gression logistique* afin de transformer notre vecteur de *logit* en vecteur de probabilit\u00e9s. Impl\u00e9mentons cette fonction."]}, {"cell_type": "code", "execution_count": null, "id": "ed93107e", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def softmax(x):\n", "    return np.exp(x)/np.exp(x).sum()\n", "\n", "x = [10, 12]\n", "print('Softmax [10, 12]:', softmax(x))\n", "\n", "x = [-10, 12]\n", "print('Softmax [-10, 12]:', softmax(x))\n", "\n", "x = [748, 750]\n", "print('Softmax [748, 750]:', softmax(x))\n"]}, {"cell_type": "markdown", "id": "2e80f050", "metadata": {}, "source": ["Le calcul de l'exponentielle de $750$ entra\u00eene \u00e0 nouveau un $\\texttt{overflow}$ et nous emp\u00eache de calculer le *softmax*. La strat\u00e9gie consiste \u00e0 r\u00e9duire la taille du plus grand nombre que notre exponentielle devra calculer de la mani\u00e8re suivante&nbsp;:\n", "\n", "$$\\text{softmax}(x)_j=\\frac{e^{x_j}}{\\sum_i e^{x_i}}=\\frac{e^{x_j}}{\\sum_i e^{x_i}}\\frac{e^{-\\text{max}(x)}}{e^{-\\text{max}(x)}}=\\frac{e^{x_j-\\text{max}(x)}}{\\sum_i e^{x_i-\\text{max}(x)}}.$$\n", "\n", "R\u00e9impl\u00e9mentons notre *softmax*."]}, {"cell_type": "code", "execution_count": null, "id": "0d0f04f0", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def softmax(x):\n", "    return np.exp(x-np.max(x))/np.exp(x-np.max(x)).sum()\n", "\n", "x = [10, 12]\n", "print('Softmax [10, 12]:', softmax(x))\n", "\n", "x = [-10, 12]\n", "print('Softmax [-10, 12]:', softmax(x))\n", "\n", "x = [748, 750]\n", "print('Softmax [748, 750]:', softmax(x))\n", "\n"]}, {"cell_type": "markdown", "id": "2bac52c9", "metadata": {}, "source": ["De tr\u00e8s nombreuses astuces de ce type existent et sont impl\u00e9ment\u00e9es dans les diff\u00e9rents *frameworks*.\n", "\n", "C'est exactement cela que nous voulons faire avec les moindres carr\u00e9s via une d\u00e9composition QR. Calculer notre estimateur des moindres carr\u00e9s est beaucoup plus stable apr\u00e8s une d\u00e9composition QR que dans sa formulation telle que nous l'avons vue."]}, {"cell_type": "markdown", "id": "ffc25fd2", "metadata": {}, "source": ["## II. D\u00e9composition QR\n", "Soit une matrice $A\\in\\mathbb{R}^{m\\times n}$. La d\u00e9composition $QR$ de la matrice $A$ est&nbsp;:\n", "\n", "$$A=QR,$$\n", "\n", "o\u00f9 $Q$ est une matrice orthogonale (par colonne si rectangulaire) et $R$ une matrice diagonale sup\u00e9rieure. On retrouve d'ailleurs parfois l'appellation \"d\u00e9composition $QU$\" o\u00f9 $U$ signifie *Upper triangular*. Une matrice orthogonale par colonne implique $Q^TQ=I$ et donc $m\\geq n$. En effet, si une famille de vecteurs est plus grande que la dimension de l'espace, alors elle est forc\u00e9ment li\u00e9e.\n", "\n", "### A. Proc\u00e9d\u00e9 de Gram-Schmidt\n", "Il existe plusieurs strat\u00e9gies permettant de r\u00e9aliser cette d\u00e9composition et nous utiliserons celle bas\u00e9e sur le proc\u00e9d\u00e9 (ou algorithme) de Gram-Schmidt. Soit $F=\\{u_1, ..., u_n\\}$ une famille de vecteurs libres. Le proc\u00e9d\u00e9 de Gram-Schmidt a pour objectif de construire une base orthonormale $B=\\{e_1, ..., e_n\\}$ \u00e0 partir de $F$.\n", "\n", "L'op\u00e9ration de base du proc\u00e9d\u00e9 de Gram-Schmidt est l'op\u00e9rateur de projection&nbsp;:\n", "\n", "$$\\textrm{proj}_u(v)=\\Pi_u(v)=\\frac{\\langle v, u\\rangle }{\\langle u, u\\rangle}u,$$\n", "\n", "o\u00f9 le vecteur $v$ est projet\u00e9 orthogonalement sur $u$.\n", "\n", "Le proc\u00e9d\u00e9 it\u00e8re sur l'ensemble des vecteurs de la famille $F$ de la mani\u00e8re suivante.\n", "\n", "\n", "1.  $v_1=u_1$ et $e_1=v_1/\\lVert v_1\\rVert_2$,\n", "2.  $v_2=u_2-\\Pi_{e_1}(u_2)$ et $e_2=v_2/\\lVert v_2\\rVert_2$,\n", "3.  ...\n", "4.  $v_n=u_n-\\sum_{i=1}^{n-1}\\Pi_{e_i}(u_n)$ et $e_n=v_n/\\lVert v_n\\rVert_2$.\n", "\n", "\n", "La famille $\\{e_1, ..., e_n\\}$ ainsi construite est une base orthonorm\u00e9e et engendre le m\u00eame sous-espace vectoriel que la famille $F$.\n", "\n", "**<span style='color:blue'> Exercice\n</span>** ", "\n", "**Compl\u00e9tez le code ci-dessous afin d'impl\u00e9menter le proc\u00e9d\u00e9 de Gram-Schmidt.**\n", "\n", "----"]}, {"cell_type": "code", "id": "c2d2280c", "metadata": {}, "source": ["import numpy as np\n", "\n", "def projector(u, v):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "def gram_schmidt(matrix):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "A = np.random.random((4, 4))\n", "\n", "Q = gram_schmidt(A)\n", "\n", "print('Une matrice identite :\\n', np.dot(Q.T, Q))\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "023b4988", "metadata": {}, "source": ["### B. D\u00e9composition QR\n", "\n", "Soit $A\\in\\mathbb{R}^{m\\times n}$ telle que les vecteurs colonnes sont libres. Notons $\\{a_1, ..., a_n\\}$ l'ensemble des vecteurs colonnes. Soit $\\{e_1, ..., e_n\\}$ une base orthonormale r\u00e9sultant du proc\u00e9d\u00e9 de Gram-Schmidt appliqu\u00e9 aux vecteurs colonnes de $A$.\n", "\n", "De mani\u00e8re assez directe, on observe que&nbsp;:\n", "\n", "$$a_1=\\langle a_1, e_1\\rangle e_1.$$ \n", "\n", "Dit autrement, $a_1$ est un vecteur co-lin\u00e9aire \u00e0 $e_1$ dont la norme est $\\langle e_1, a_1\\rangle$ (sachant que $e_1$ est unitaire). Le vecteur $a_2$ est un peu plus complexe \u00e0 reconstruire&nbsp;: \n", "\n", "$$a_2=\\langle a_2, e_2\\rangle e_2+\\langle a_2, e_1\\rangle e_1.$$ \n", "\n", "Autrement dit, $a_2$ est une combinaison lin\u00e9aire de $e_1$ et $e_2$, ce qui est logique puisque $e_2$ est construit en retirant la composante non orthogonale \u00e0 $e_1$ de $a_2$.\n", "On r\u00e9it\u00e8re l'op\u00e9ration jusqu'\u00e0 $a_n=\\sum_i \\langle e_i, a_n\\rangle e_i$.\n", "\n", "En notant&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    Q=[e_1, ..., e_n]\\textrm{ et }R=\\begin{bmatrix}\n", "        \\langle e_1, a_1\\rangle & \\langle e_1, a_2\\rangle & \\langle e_1, a_3\\rangle & \\ldots&\\langle e_1, a_n\\rangle\\\\\n", "        0 & \\langle e_2, a_2\\rangle & \\langle e_2, a_3\\rangle& \\ldots&\\langle e_2, a_n\\rangle\\\\\n", "        0 &0 & \\langle e_3, a_3\\rangle& \\ldots&\\langle e_3, a_n\\rangle\\\\\n", "        0 & 0 & 0 & \\ddots & \\vdots\\\\\n", "        0 & 0 & 0 & \\ldots & \\langle e_n, a_n\\rangle\n", "        \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "on retrouve bien $A=QR$.\n", "\n", "### C. Exemple\n", "\n", "Soit la matrice suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    A=\\begin{bmatrix}\n", "    1&-1\\\\\n", "    2&0\\\\\n", "    2&2\n", "    \\end{bmatrix}=[a_1, a_2]\n", "\\end{aligned}$$\n", "\n", "Commen\u00e7ons la proc\u00e9dure de Gram-Schmidt. On note&nbsp;:\n", "\n", "$$v_1=a_1\\textrm{ et }e_1=v_1/\\lVert v_1\\rVert_2=\\Big[\\frac{1}{3},\\frac{2}{3}, \\frac{2}{3}\\Big]^T$$\n", "\n", "Et&nbsp;:\n", "\n", "$$v_2=a_2-\\Pi_{e_1}(a_2)\\textrm{ et }e_2=v_2/\\lVert v_2\\rVert_2=\\Big[-\\frac{2}{3},-\\frac{1}{3}, \\frac{2}{3}\\Big]^T$$\n", "\n", "On v\u00e9rifie assez bien que $e_1$ et $e_2$ sont orthogonaux et unitaires.\n", "Calculons maintenant les produits scalaires&nbsp;:\n", "\n", "$$\\langle e_1, a_1\\rangle=3,\\ \\langle e_1, a_2\\rangle=1\\textrm{ et }\\langle e_2, a_2\\rangle=2$$\n", "\n", "Nous avons donc\n", "\n", "$$\\begin{aligned}\n", "    Q=\\begin{bmatrix}\n", "    1/3&-2/3\\\\\n", "    2/3&-1/3\\\\\n", "    2/3&2/3\n", "    \\end{bmatrix}\\text{ et }R=\\begin{bmatrix}\n", "    3&1\\\\\n", "    0&2\n", "    \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "On v\u00e9rifie facilement qu'on a bien l'\u00e9galit\u00e9 $A=QR$.\n", "\n", "\n", "**<span style='color:blue'> Exercice\n</span>** ", "\n", "**Compl\u00e9tez le code ci-dessous en r\u00e9-utilisant votre impl\u00e9mentation du proc\u00e9d\u00e9 de Gram-Schmidt afin d'obtenir une d\u00e9composition QR.**\n", "\n", "----"]}, {"cell_type": "code", "id": "61995e47", "metadata": {}, "source": ["def qr(matrix):\n", "    ####### Complete this part ######## or die ####################\n", "    ...\n", "    ...\n", "    return ...\n", "    ###############################################################\n", "\n", "A = np.array([[1, -1], [2, 0], [2, 2]])\n", "\n", "Q, R = qr(A)\n", "print('Notre matrice initiale :\\n', A)\n", "print('Notre matrice initiale reconstruite :\\n', np.dot(Q, R))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "9c099bd2", "metadata": {}, "source": ["## III. Application aux moindres carr\u00e9s\n", "### A. Notre estimation via une d\u00e9composition QR\n", "\n", "Soit $X\\in\\mathbb{R}^{n\\times d}$, $y\\in\\mathbb{R}^n$ et $\\beta\\in\\mathbb{R}^d$. Notre objectif est de r\u00e9soudre le probl\u00e8me d'optimisation suivant&nbsp;\n", "\n", "$$\\hat{\\beta}=\\text{argmin}_{\\beta\\in\\mathbb{R}^d}\\lVert X\\beta-y\\rVert_2^2.$$\n", "\n", "Nous avons d\u00e9j\u00e0 vu que si $X^TX$ est inversible, alors nous avons la solution analytique suivante&nbsp;:\n", "\n", "$$\\hat{\\beta}=(X^TX)^{-1}X^Ty.$$\n", "\n", "Consid\u00e9rons maintenant la d\u00e9composition $QR$ de la matrice $X$ (i.e. $X=QR$). Nous avons ainsi&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    \\hat{\\beta}&=(X^TX)^{-1}X^Ty\\\\\n", "    &=((QR)^TQR)^{-1}(QR)^Ty\\\\\n", "    &= ((R^TQ^TQR)^{-1}R^TQ^Ty\\\\\n", "    &=(R^TR)^{-1}R^TQ^Ty\\\\\n", "    &=R^{-1}(R^T)^{-1}R^TQ^Ty\\\\\n", "    &=R^{-1}Q^Ty.\n", "\\end{aligned}$$\n", "\n", "Rappellons que si $A$ et $B$ sont deux matrices inversibles, nous avons $(AB)^{-1}=B^{-1}A^{-1}$.\n", "\n", "### B. Exercice\n", "\n", "Soit la matrice suivante&nbsp;:\n", "\n", "$$\\begin{aligned}\n", "    X=\\begin{bmatrix}\n", "    1&-1\\\\\n", "    0&10^{-5}\\\\\n", "    0&0\n", "    \\end{bmatrix}\\textrm{ et }Y=\\begin{bmatrix}\n", "    0\\\\\n", "    10^{-5}\\\\\n", "    0\n", "    \\end{bmatrix}\n", "    \\end{aligned}$$\n", "    \n", "\n", "**<span style='color:blue'> Exercice 1\n</span>** ", "\n", "**Supposons que l'algorithme tourne sur une machine o\u00f9 les nombres sont arrondis apr\u00e8s 8 d\u00e9cimales (i.e. si $|x|<10^{-8}$ alors $x:=0$). Calculez l'estimateur des moindres carr\u00e9s SANS passer par une d\u00e9composition QR.**\n", "\n", "----", "\n", "**<span style='color:blue'> Exercice 2\n</span>** ", "\n", "**Supposons que l'algorithme tourne sur une machine o\u00f9 les nombres sont arrondis apr\u00e8s 8 d\u00e9cimales (i.e. si $|x|<10^{-8}$ alors $x:=0$). Calculez l'estimateur des moindres carr\u00e9s AVEC une d\u00e9composition QR.**\n", "\n", "----", "\n", "\n", "**<span style='color:blue'> Exercice 3\n</span>** ", "\n", "**Impl\u00e9mentez les deux strat\u00e9gies pr\u00e9c\u00e9dentes en utilisant $\\texttt{numpy}$. Que constatez-vous ?**\n", "\n", "----"]}, {"cell_type": "code", "id": "505b74e5", "metadata": {}, "source": ["import numpy as np\n", "\n", "X = np.array([[1, -1], [0, 1e-5], [0, 0]])\n", "y = np.array([[0], [1e-5], [0]])\n", "\n", "####### Complete this part ######## or die ####################\n", "...\n", "...\n", "...\n", "###############################################################\n", "\n", "print(\"Notre estimateur avec la m\u00e9thode classique :\\n\", beta_est)\n", "print(\"Notre estimateur avec la m\u00e9thode QR :\\n\", beta_qr_est)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "c3e475c7", "metadata": {}, "source": ["Cette exemple simple montre d\u00e9j\u00e0 les avantages de la d\u00e9composition QR dans le cadre des moindres carr\u00e9s. On imagine sans mal son int\u00e9r\u00eat dans des exemples beaucoup plus compliqu\u00e9s o\u00f9 les d\u00e9pendances lin\u00e9aires sont peut-\u00eatre plus difficiles \u00e0 discerner au milieu des perturbations et du bruit."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.2"}}, "nbformat": 4, "nbformat_minor": 5}