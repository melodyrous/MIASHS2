{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "03_methodes_ensembliste.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "collectible-validation"
      },
      "source": [
        "# Méthodes ensemblistes ☕️☕️"
      ],
      "id": "collectible-validation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arbitrary-beijing"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Soit $\\mathcal{X}$ notre espace d'entrée et $\\mathcal{Y}$ notre espace de sortie. Soit $X, Y$ deux variables aléatoires sur $\\mathcal{X}$ et $\\mathcal{Y}$ et soit $\\mathbb{P}$ leur mesure jointe. Notre objectif est de trouver une application $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ qui minimise une certaine erreur qu'on notera $L$. N'ayant pas accès aux variables aléatoires $X$ et $Y$, nous collectons un jeu de données $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}\\sim\\mathbb{P}^n$ (par exemple en récupérant des images sur internet) et nous construisons un risque empirique :\n",
        "\n",
        "$$L_n(h)=\\frac{1}{n}\\sum_{i}\\ell(h(x_i), y_i),$$\n",
        "\n",
        "où $\\ell$ définit une erreur élémentaire (i.e. pour une unique prédiction). Comme nous l'avons vu, ce risque empirique $L_n$ est un estimateur de $L$. La fonction $h$ est ainsi construite en utilisant le jeu de données $S_n$.\n",
        "\n"
      ],
      "id": "arbitrary-beijing"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "democratic-annotation"
      },
      "source": [
        "## I. L'approche naïve\n",
        "\n",
        "L'approche la plus simple lorsqu'on cherche à combiner plusieurs modèles consiste à moyenner leur prédiction. Dans le cas de la régression, la prédiction est la moyenne traditionnelle. Dans le cas de la classification, on utilisera un vote à la majorité simple.\n",
        "\n",
        "---\n",
        "\n",
        "Soit $\\mathcal{Y}\\subseteq\\mathbb{R}$ l'espace des labels et $\\mathcal{X}$ l'espace de nos données d'entrée. Considérons une famille de prédicteurs $\\{h_j\\}_{j\\leq m}$, l'agrégation de ces derniers se fait de la manière la plus naïve qui soit (comme nous l'avons vu au-dessus)&nbsp;:\n",
        "\n",
        "$$\\hat{y}=\\frac{1}{m}\\sum_j h_j(x).$$\n",
        "\n",
        "Supposons que nous ayons pour un prédicteur donnée $\\hat{y}_{ij} = y(x_j)+\\epsilon_{ij}$ où la prédiction se fait à un bruit près qui dépend de l'échantillon mais aussi du prédicteur. Nous avons évidemment la relation suivante&nbsp;:\n",
        "\n",
        "$$\\mathbb{E}\\big[(\\hat{y}_j(X)-y(X))^2\\big]=\\mathbb{E}\\big[\\epsilon_{j}^2\\big],$$\n",
        "\n",
        "où $\\epsilon_j$ dépend de $X$. De manière similaire, si cette fois-ci nous considérons l'agrégation de nos prédicteurs, nous avons&nbsp;:\n",
        "\n",
        "$$\\mathbb{E}\\big[(y(X)-\\frac{1}{m}\\sum_j \\hat{y_j}(X))^2\\big]=\\mathbb{E}\\big[(\\frac{1}{m}\\sum_j\\epsilon_{j})^2\\big].$$\n",
        "\n",
        "Si on suppose (ce qui n'est pas vraiment le cas en pratique)&nbsp;:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\mathbb{E}\\big[\\epsilon_j\\big]&=0\\\\\n",
        "\\mathbb{E}\\big[\\epsilon_j\\epsilon_k\\big]&=0,\\ j\\neq k,\n",
        "\\end{aligned}$$\n",
        "nous avons&nbsp;:\n",
        "\n",
        "$$\\mathbb{E}\\big[(\\frac{1}{m}\\sum_j\\epsilon_{j})^2\\big]=\\frac{1}{m^2}\\sum_i\\mathbb{E}\\big[\\epsilon^2\\big]=\\frac{1}{m}\\mathbb{E}_{\\bar{\\epsilon}},$$\n",
        "\n",
        "où $\\mathbb{E}_{\\bar{\\epsilon}}$ indique l'erreur moyenne de nos prédicteurs. L'erreur diminue au fur et à mesure où on ajoute des prédicteurs.\n",
        "\n",
        "---"
      ],
      "id": "democratic-annotation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "id": "productive-playlist",
        "outputId": "a9543ff7-5bc1-4670-dd36-2f20ec68b272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_digits()\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, data.images, data.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)"
      ],
      "id": "productive-playlist",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaningful-bangladesh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data.data / data.data.max()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"
      ],
      "id": "meaningful-bangladesh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "available-lambda"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from scipy import stats"
      ],
      "id": "available-lambda",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjksoyhiMLu9",
        "outputId": "23ae84ac-4dca-4751-f770-e23780cff3f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train"
      ],
      "id": "MjksoyhiMLu9",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 6, 3, 0, 5, 7, 4, 1, 6, 6, 2, 8, 6, 0, 9, 0, 2, 5, 0, 4, 4, 2,\n",
              "       1, 0, 2, 4, 0, 1, 0, 6, 2, 6, 0, 0, 2, 1, 2, 8, 8, 3, 2, 4, 0, 8,\n",
              "       5, 3, 1, 3, 4, 6, 3, 7, 8, 0, 7, 0, 7, 0, 9, 4, 8, 8, 5, 0, 9, 7,\n",
              "       7, 4, 4, 5, 7, 0, 8, 2, 2, 8, 3, 6, 9, 8, 0, 0, 2, 6, 9, 8, 8, 7,\n",
              "       1, 8, 3, 2, 7, 5, 1, 1, 2, 0, 6, 0, 1, 2, 6, 6, 6, 1, 6, 0, 8, 4,\n",
              "       0, 4, 8, 7, 5, 2, 3, 5, 6, 0, 6, 3, 7, 0, 3, 0, 2, 1, 6, 2, 5, 1,\n",
              "       0, 3, 3, 2, 6, 7, 7, 2, 2, 6, 8, 8, 8, 1, 9, 0, 8, 2, 5, 5, 7, 9,\n",
              "       3, 9, 0, 8, 1, 4, 5, 3, 0, 5, 6, 1, 8, 5, 7, 3, 4, 1, 1, 1, 5, 9,\n",
              "       5, 5, 9, 0, 6, 8, 1, 4, 6, 5, 0, 8, 7, 8, 0, 6, 0, 6, 9, 8, 9, 9,\n",
              "       8, 8, 1, 7, 1, 3, 8, 0, 7, 2, 0, 9, 4, 6, 7, 5, 9, 7, 0, 4, 9, 2,\n",
              "       6, 4, 7, 9, 9, 8, 6, 2, 4, 8, 0, 4, 2, 9, 9, 4, 4, 4, 5, 7, 5, 8,\n",
              "       6, 7, 6, 0, 2, 2, 5, 1, 5, 7, 8, 7, 2, 8, 9, 3, 3, 5, 6, 1, 8, 4,\n",
              "       6, 9, 6, 3, 2, 1, 1, 1, 2, 8, 4, 9, 9, 6, 4, 0, 5, 3, 5, 5, 3, 0,\n",
              "       6, 0, 1, 1, 3, 5, 6, 2, 6, 7, 2, 0, 8, 6, 7, 6, 0, 0, 6, 2, 0, 4,\n",
              "       6, 0, 7, 4, 3, 1, 5, 7, 7, 0, 1, 6, 3, 1, 5, 6, 9, 9, 5, 0, 1, 5,\n",
              "       2, 1, 5, 7, 2, 7, 2, 6, 3, 2, 2, 4, 1, 3, 2, 6, 8, 3, 1, 4, 3, 5,\n",
              "       3, 5, 2, 4, 7, 3, 4, 7, 4, 6, 6, 7, 2, 0, 7, 0, 8, 5, 9, 2, 7, 9,\n",
              "       6, 6, 9, 6, 3, 6, 2, 2, 2, 3, 6, 4, 8, 0, 7, 3, 7, 6, 6, 2, 6, 0,\n",
              "       2, 4, 0, 4, 0, 1, 0, 1, 9, 8, 0, 7, 9, 0, 8, 3, 5, 5, 0, 1, 8, 3,\n",
              "       4, 7, 4, 7, 2, 2, 5, 0, 1, 6, 3, 5, 6, 3, 4, 0, 6, 6, 0, 3, 1, 5,\n",
              "       5, 6, 5, 3, 6, 9, 9, 5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71e71cd1"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Complétez la méthode $\\texttt{predict}$ de la classe suivante afin de faire un vote à la majorité simple pour combiner plusieurs modèles.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "71e71cd1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cc7f75b",
        "outputId": "87eace9a-9915-4c2f-bcac-45c39d43995b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class MajorityVoting(object):\n",
        "    def __init__(self, *models):\n",
        "        self.models = models\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        for m in self.models:\n",
        "            m.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        ####### Complete this part ######## or die ####################\n",
        "        pred = np.zeros((len(self.models), X.shape[0]))\n",
        "        for i, m in enumerate(self.models):\n",
        "          pred[i] = m.predict(X)\n",
        "        return stats.mode(pred).mode\n",
        "\n",
        "        ###############################################################\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        scores = {'model': [], 'vote': None}\n",
        "        for m in self.models:\n",
        "            scores['model'].append(m.score(X, y))\n",
        "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n",
        "        return scores\n",
        "            \n",
        "model = MajorityVoting(\n",
        "    LogisticRegression(max_iter=5000), \n",
        "    DecisionTreeClassifier(),\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "scores = model.score(X_test, y_test)\n",
        "\n",
        "print('Nos modèles atteignent:', scores['model'])\n",
        "print('Notre agrégation atteint:', scores['vote'])\n"
      ],
      "id": "7cc7f75b",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nos modèles atteignent: [0.9421364985163204, 0.776706231454006, 0.951780415430267]\n",
            "Notre agrégation atteint: 0.9480712166172107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dressed-russia"
      },
      "source": [
        "On peut imaginer que les modèles se trompent sur les mêmes images et non de manière aléatoire. C'est par exemple le cas si les modèles se trompent lorsqu'un chiffre est mal dessiné et ressemble à un autre chiffre : tous les modèles vont faire la même erreur. Ainsi, le meilleur modèle est le meilleur modèle et non le vote. À l'inverse, si nos différents modèles ont tendance à faire des erreurs différentes, alors l'agrégation permettant de gagner en score.\n",
        "\n",
        "Considérons maintenant le jeu de données suivant."
      ],
      "id": "dressed-russia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "subtle-magnitude"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "data = load_boston()"
      ],
      "id": "subtle-magnitude",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resident-copyright"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data.data/data.data.max(axis=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"
      ],
      "id": "resident-copyright",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DidDhIdTzGi",
        "outputId": "78d8eb5b-5e47-42ba-b77c-90fa56f1b2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_test.shape"
      ],
      "id": "6DidDhIdTzGi",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(380, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "north-borough"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "id": "north-borough",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbdb9cb5"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Complétez la méthode $\\texttt{predict}$ de la classe suivante afin de faire une moyenne pour combiner plusieurs modèles.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "fbdb9cb5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d73d2095",
        "outputId": "845df03d-cd8a-44bd-fc4b-6795734c201a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class AverageVoting(object):\n",
        "    def __init__(self, *models):\n",
        "        self.models = models\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        for m in self.models:\n",
        "            m.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        ####### Complete this part ######## or die ####################*\n",
        "        pred = np.zeros((len(self.models),len(X)))\n",
        "        for i, m in enumerate(self.models):\n",
        "          pred[i] = m.predict(X)\n",
        "        return np.mean(pred, axis=0)\n",
        "\n",
        "        ###############################################################\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        scores = {'model': [], 'vote': None}\n",
        "        for m in self.models:\n",
        "            scores['model'].append(mean_squared_error(y, m.predict(X)))\n",
        "        scores['vote'] = mean_squared_error(y, self.predict(X))\n",
        "        return scores\n",
        "            \n",
        "model = AverageVoting(\n",
        "    LinearRegression(), \n",
        "    DecisionTreeRegressor(),\n",
        "    KNeighborsRegressor()\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "scores = model.score(X_test, y_test)\n",
        "\n",
        "print('Nos modèles atteignent:', scores['model'])\n",
        "print('Notre agrégation atteint:', scores['vote'])\n"
      ],
      "id": "d73d2095",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nos modèles atteignent: [23.42635932052333, 38.59068421052631, 33.130238947368426]\n",
            "Notre agrégation atteint: 20.534940116967498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checked-daily"
      },
      "source": [
        "On observe cette fois-ci un gain clair des performances. Les différents modèles doivent se tromper d'une manière aléatoire et répartie autour de la moyenne. L'agrégation rend ce résultat plus stable.\n",
        "\n",
        "Cette manière d'agréger des votes est effectives mais restent \"naïve\". Nous allons voir que nous pouvons formaliser tout cela dans le cadre du *framework* probabiliste."
      ],
      "id": "checked-daily"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrong-vacuum"
      },
      "source": [
        "## II. Bayesian Model Averaging\n",
        "\n",
        "Notons $z_i=(x_i, y_i)$ et une famille de $M$ modèles probabilistes (e.g. régression logistique) où chaque modèle est noté $\\mathcal{M}_j$. Notons :\n",
        "\n",
        "$$p(y_i|x_i, \\mathcal{M}_j),$$\n",
        "\n",
        "la densité d'un point de $\\mathcal{Y}$ relativement au modèle $\\mathcal{M}_j$ et à une observation $x_i\\in\\mathcal{X}$. Notre objectif est de déterminer dans un premier temps la \"qualité\" d'un modèle en tenant compte de notre point de vu *a priori* ainsi que des données que nous avons pu observer $S_n$. Notons ainsi $p(\\mathcal{M}_j)$ notre probabilité *a priori* sur le modèle $\\mathcal{M}_j$. Celle-ci peut favoriser certaines solutions parcimonieuses ou bien être uniforme et ne favoriser aucun modèle. En appliquant la règle de Bayes, nous obtenons (modele bayesien):\n",
        "\n",
        "$$p(\\mathcal{M}_j|X, \\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|X, \\mathcal{M}_j)p(\\mathcal{M}_j)}{p(\\boldsymbol{y}| X)},$$\n",
        "\n",
        "où $S_n=(\\boldsymbol{y}, X)$ et où nous avons : (vraissemblance)\n",
        "\n",
        "$$p(\\boldsymbol{y}|X\\mathcal{M}_j)=\\prod_i p(y_i|x_i, \\mathcal{M}_j),$$\n",
        "\n",
        "ainsi que : (evidence)\n",
        "\n",
        "$$p(\\boldsymbol{y}| X)=\\sum_j p(\\boldsymbol{y}|X, \\mathcal{M}_j)p(\\mathcal{M}_j).$$\n",
        "\n",
        "Nous avons bien ici toutes les informations nous permettant d'évaluer de manière Bayésienne la qualité de nos différents modèles. Cependant, en pratique, les variations pourraient très bien dépendre d'un tirage particulier de nos données. L'idée derrière le *Bayesian Model Averaging* consiste à considérer TOUS les modèles mais à les pondérer par leur qualité. Cela ne se fait pas au doigt mouillé, mais en utilisant encore une fois le *framework* probabiliste :\n",
        "\n",
        "$$p(y_\\text{new}|x_\\text{new}, S_n)=\\sum_j p(y_\\text{new}|x_\\text{new}, \\mathcal{M}_j, S_n)p(\\mathcal{M}_j|S_n)$$"
      ],
      "id": "wrong-vacuum"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virgin-dictionary"
      },
      "source": [
        "**Application a une famille de régression logistique.**\n",
        "\n",
        "Dans le cas de la régression logistique, nous avons la vraisemblance suivante :\n",
        "\n",
        "$$\\mathcal{L}(\\theta_j)=\\prod_i p(y_i|x_i,\\theta_j)$$\n",
        "\n",
        "où\n",
        "\n",
        "$$p(y_i|x_i, \\theta_j)=\\sigma_j(x_i)^y_i(1-\\sigma_j(x_i))^{1-y_i}$$\n",
        "\n",
        "et\n",
        "\n",
        "$$\\sigma_j(x)=(1+e^{-\\langle\\theta_j,x\\rangle})^{-1}.$$\n",
        "\n",
        "Ainsi, la *posterior* de notre modèle $\\theta_j$ après avoir observé notre jeu de données est donnée par :\n",
        "\n",
        "$$p(\\theta_j|S_n)\\propto \\prod_i p(y_i|x_i,\\theta_j)p(\\theta_j),$$\n",
        "\n",
        "où le symbole $\\propto$ indique la proportionnalité."
      ],
      "id": "virgin-dictionary"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d387923"
      },
      "source": [
        "Nous allons construire dans cette exercice un modèle de classification d'images de chiffres à partir de régressions logistiques. La particularité sera que chaque régression logistique ne verra qu'une partie de l'image tirée aléatoirement."
      ],
      "id": "8d387923"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numerous-darkness"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "data = load_digits()"
      ],
      "id": "numerous-darkness",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nominated-opera"
      },
      "source": [
        "Affichons le jeu de données ainsi qu'un exemple de ce que pourrait voir un modèle."
      ],
      "id": "nominated-opera"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chief-traveler",
        "outputId": "919a2d42-0a56-4ab5-ec52-6b17e8c6bb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, data.images, data.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)\n",
        "\n",
        "mask = np.random.binomial(n=1, p=0.4, size=data.images[0].shape).astype(bool)\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, data.images, data.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(\n",
        "        np.ma.masked_array(image, mask=mask), \n",
        "        cmap=plt.cm.gray_r, \n",
        "        interpolation='nearest'\n",
        "    )\n",
        "    ax.set_title('Training: %i' % label)"
      ],
      "id": "chief-traveler",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37876ea8"
      },
      "source": [
        "Construisons notre jeu d'apprentissage ainsi que notre jeu de test."
      ],
      "id": "37876ea8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grave-appendix"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data.data / data.data.max()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size=0.75)"
      ],
      "id": "grave-appendix",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfccc75"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Complétez les méthodes $\\texttt{posterior_}$, $\\texttt{fit}$ et $\\texttt{predict}$ afin de reproduire le *framework* que nous avons introduit au-dessus.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "8cfccc75"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piano-pendant"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp"
      ],
      "id": "piano-pendant",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c382fd0f",
        "outputId": "78832577-e303-4e21-a5dc-d8cd50e5b2c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class BayesianLogisticAveraging(object):\n",
        "    def __init__(self, dim=64, p=0.5, nb_models=50, max_iter=5000):\n",
        "        self.mask = np.random.binomial(n=1, p=p, size=(nb_models, dim)).astype(bool)\n",
        "        self.nb_models = nb_models\n",
        "        self.models = []\n",
        "        self.posterior = []\n",
        "        self.prior = 1./nb_models\n",
        "        self.max_iter = max_iter\n",
        "        \n",
        "    def posterior_(self, i, X, y):\n",
        "        ####### Complete this part ######## or die ####################\n",
        "        p = self.models[i].predict_proba(X)\n",
        "\n",
        "        return np.log(p[np.arange(len(p)), y]).sum() + np.log(self.prior)\n",
        "        ###############################################################\n",
        "        \n",
        "        \n",
        "    def fit(self, X_train, y_train):\n",
        "        ####### Complete this part ######## or die ####################\n",
        "        for _ in range(self.nb_models):\n",
        "            model = LogisticRegression(max_iter=self.max_iter)\n",
        "            X = X_train[:, self.mask[_]]\n",
        "            model.fit(X, y_train)\n",
        "            self.models.append(model)\n",
        "            self.posterior.append(\n",
        "                self.posterior_(_, X, y_train)\n",
        "            )\n",
        "        ###############################################################\n",
        "        \n",
        "    def individual_scores(self, X, y):\n",
        "        scores = []\n",
        "        for _ in range(len(self.models)):\n",
        "            scores.append(self.models[_].score(X[:, self.mask[_]], y))\n",
        "        print('Best model:', np.max(scores), ':: Worst model:', np.min(scores))\n",
        "            \n",
        "    def predict(self, X):\n",
        "        ####### Complete this part ######## or die ####################\n",
        "        predictions = []\n",
        "        for i, model in enumerate(self.models):\n",
        "            predictions.append(\n",
        "                np.log(model.predict_proba(X[:, self.mask[i]]))+self.posterior[i]\n",
        "            )\n",
        "        predictions = np.array(predictions)\n",
        "        return logsumexp(predictions, axis=0).argmax(axis=1)\n",
        "        ###############################################################\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        scores = {'model': [], 'vote': None}\n",
        "        for i, m in enumerate(self.models):\n",
        "            scores['model'].append(m.score(X[:, self.mask[i]], y))\n",
        "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n",
        "        return scores\n",
        "\n",
        "model = BayesianLogisticAveraging()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.individual_scores(X_test, y_test)\n",
        "\n",
        "scores = model.score(X_test, y_test)\n",
        "\n",
        "print('Nos modèles atteignent:\\n -', '\\n - '.join([str(i) for i in scores['model']]))\n",
        "print('Notre agrégation atteint:', scores['vote'])\n",
        "\n"
      ],
      "id": "c382fd0f",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: 0.9206231454005934 :: Worst model: 0.7789317507418397\n",
            "Nos modèles atteignent:\n",
            " - 0.8887240356083086\n",
            " - 0.9094955489614244\n",
            " - 0.887240356083086\n",
            " - 0.9102373887240356\n",
            " - 0.8954005934718101\n",
            " - 0.8954005934718101\n",
            " - 0.8093471810089021\n",
            " - 0.8716617210682492\n",
            " - 0.8894658753709199\n",
            " - 0.9065281899109793\n",
            " - 0.8501483679525222\n",
            " - 0.9065281899109793\n",
            " - 0.836053412462908\n",
            " - 0.8501483679525222\n",
            " - 0.9072700296735905\n",
            " - 0.8761127596439169\n",
            " - 0.8916913946587537\n",
            " - 0.8946587537091988\n",
            " - 0.8909495548961425\n",
            " - 0.9028189910979229\n",
            " - 0.8835311572700296\n",
            " - 0.8301186943620178\n",
            " - 0.8857566765578635\n",
            " - 0.8508902077151336\n",
            " - 0.8902077151335311\n",
            " - 0.8939169139465876\n",
            " - 0.9183976261127597\n",
            " - 0.8353115727002968\n",
            " - 0.9013353115727003\n",
            " - 0.8775964391691394\n",
            " - 0.7789317507418397\n",
            " - 0.8330860534124629\n",
            " - 0.8775964391691394\n",
            " - 0.8219584569732937\n",
            " - 0.8827893175074184\n",
            " - 0.9206231454005934\n",
            " - 0.8523738872403561\n",
            " - 0.8538575667655787\n",
            " - 0.9013353115727003\n",
            " - 0.9117210682492581\n",
            " - 0.9146884272997032\n",
            " - 0.892433234421365\n",
            " - 0.8545994065281899\n",
            " - 0.8464391691394659\n",
            " - 0.9169139465875371\n",
            " - 0.8991097922848664\n",
            " - 0.8716617210682492\n",
            " - 0.8583086053412463\n",
            " - 0.8961424332344213\n",
            " - 0.8976261127596439\n",
            "Notre agrégation atteint: 0.9287833827893175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50289e90"
      },
      "source": [
        "De la même manière que précédemment un gain est observé si les erreurs sont indépendantes. De plus, il est important d'avoir une bonne estimation des probabilités conditionnelles afin que la vraisemblance soit un bon indicateur de la qualité de notre modèle. Une stratégie alternative, souvent utilisée en *deep learning* consiste pour chaque prédiction à favoriser le modèle dont les scores de prédictions sont les plus élevés."
      ],
      "id": "50289e90"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "089609b2"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Complétez la méthodes $\\texttt{predict}$ afin de que la prédiction choisie soit celle du modèle ayant le score de prédiction le plus élevé.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "089609b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da09d3c0"
      },
      "source": [
        "class LogisticMaxPooling(object):\n",
        "    def __init__(self, dim=64, p=0.5, nb_models=50, max_iter=5000):\n",
        "        self.mask = np.random.binomial(n=1, p=p, size=(nb_models, dim)).astype(bool)\n",
        "        self.nb_models = nb_models\n",
        "        self.models = []\n",
        "        self.max_iter = max_iter\n",
        "        \n",
        "        \n",
        "    def fit(self, X_train, y_train):\n",
        "        for _ in range(self.nb_models):\n",
        "            model = LogisticRegression(max_iter=self.max_iter)\n",
        "            X = X_train[:, self.mask[_]]\n",
        "            model.fit(X, y_train)\n",
        "            self.models.append(model)\n",
        "            \n",
        "    def individual_scores(self, X, y):\n",
        "        scores = []\n",
        "        for _ in range(len(self.models)):\n",
        "            scores.append(self.models[_].score(X[:, self.mask[_]], y))\n",
        "        print('Best model:', np.max(scores), ':: Worst model:', np.min(scores))\n",
        "            \n",
        "    def predict(self, X):\n",
        "        ####### Complete this part ######## or die ####################\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "        return ...\n",
        "        ###############################################################\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        scores = {'model': [], 'vote': None}\n",
        "        for i, m in enumerate(self.models):\n",
        "            scores['model'].append(m.score(X[:, self.mask[i]], y))\n",
        "        scores['vote'] = (self.predict(X) == y).astype(int).sum()/len(X)\n",
        "        return scores\n",
        "\n",
        "model = LogisticMaxPooling()\n",
        "model.fit(X_train, y_train)\n",
        "model.individual_scores(X_test, y_test)\n",
        "model.predict(X_test)\n",
        "\n",
        "scores = model.score(X_test, y_test)\n",
        "\n",
        "print('Nos modèles atteignent:\\n -', '\\n - '.join([str(i) for i in scores['model']]))\n",
        "print('Notre agrégation atteint:', scores['vote'])\n"
      ],
      "id": "da09d3c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dependent-faculty"
      },
      "source": [
        "Le principe du *Bayesian Model Averaging* se généralise bien sûr lorsqu'on a une quantité infinie de modèles et la *posterior* prédictive s'écrit :\n",
        "\n",
        "$$p(y|x_\\text{new}, S_n)=\\int p(y|x_\\text{new}, \\mathcal{M}, S_n)p(\\mathcal{M}|S_n)dM.$$\n",
        "\n",
        "Cette stratégie demande un effort analytique cependant beaucoup plus intense. La séquence \"Bayésienne linear regression\" illustrera notamment cette idée dans le cas de la régression linéaire et pointera du doigt son lien avec la notion de régularisation.\n"
      ],
      "id": "dependent-faculty"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3087d6e7"
      },
      "source": [
        "## III. Bagging\n",
        "\n",
        "\n",
        "La difficulté de considérer plusieurs modèles est souvent que pour une classe de modèles donnée, un même jeu d'apprentissage implique une même solution et combiner plusieurs fois la même chose ne sert à rien. L'idée du bagging est d'agréger plusieurs modèles en trouvant justement une stratégie pour créer de la variabilité entre ces derniers. L'agrégation se fait de la manière la plus naïve possible en moyennant les prédictions comme indiqué au-dessus.\n",
        "\n",
        "La stratégie utilisée pour créer de la variabilité est de passer par un jeu de données *bootstrap*. Soit $S_n=\\{(X_i, Y_i)\\}_{i\\leq n}$ un jeu de données de taille $n$. L'idée va être de construire $m$ nouveaux jeux de données (pour chacun de nos modèles) à partir de $S_n$ en tirant $n$ points aléatoirement dans $S_n$ **avec** remise. Ainsi, certains points apparaîtront en double, et d'autres seront absents."
      ],
      "id": "3087d6e7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32f8673e"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "id": "32f8673e",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d67e461f"
      },
      "source": [
        "Considérons la fonction suivantes&nbsp;:"
      ],
      "id": "d67e461f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "291abb66"
      },
      "source": [
        "def f(x):\n",
        "    x = x.ravel()\n",
        "    return np.exp(-(x-5) ** 2) + 2 * np.exp(-(x - 7) ** 2) + np.exp(-(x + 2) ** 2)"
      ],
      "id": "291abb66",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afc0b84d"
      },
      "source": [
        "def generate(n_samples, noise=0.1, n_repeat=1):\n",
        "    X = np.random.rand(n_samples) * 10\n",
        "    X = np.sort(X)\n",
        "\n",
        "    if n_repeat == 1:\n",
        "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
        "    else:\n",
        "        y = np.zeros((n_samples, n_repeat))\n",
        "\n",
        "        for i in range(n_repeat):\n",
        "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
        "\n",
        "    X = X.reshape((n_samples, 1))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "n_repeat=50\n",
        "\n",
        "X_test, y_test = generate(n_samples=1000, n_repeat=n_repeat)\n",
        "\n",
        "for i in range(n_repeat):\n",
        "    X, y = generate(n_samples=50)\n",
        "    X_train.append(X)\n",
        "    y_train.append(y)"
      ],
      "id": "afc0b84d",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397d48ab"
      },
      "source": [
        "Nous avons vu dans la séquence sur la régression linéaire que nous pouvions décomposer notre erreur entre une composante de biais et une composante de variance&nbsp;:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\mathbb{E}\\big[(y-\\hat{f}(x))^2\\big]=\\sigma^2+\\text{Var}\\big(\\hat{f}\\big)+\\text{bias}(\\hat{f})^2.\\end{aligned}$$\n",
        "\n",
        "L'objectif ci-dessous va être de comparer les différentes briques de notre erreur. Pour cela, nous devons trouver une stratégie pour estimer chacune de ces quantités.\n",
        "\n",
        "**<span style='color:blue'> Question</span>** \n",
        "**À votre avis, comment estimer empiriquement l'erreur totale ainsi que chacune des briques de l'erreur totale ?**\n",
        "\n",
        "\n",
        "\n",
        " ----\n",
        "\n",
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Complétez le code suivant afin d'entraîner un arbre et un baggin construit à partir d'arbres et avec 100 estimateurs.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "397d48ab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fd803b",
        "outputId": "fc3a8dc2-465c-44ff-dfbd-97eb2bd633f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "####### Complete this part ######## or die ####################\n",
        "estimators = {\n",
        "    \"Tree\": DecisionTreeRegressor(),\n",
        "    \"Bagging\": BaggingRegressor(DecisionTreeRegressor(), n_estimators=100)\n",
        "}\n",
        "###############################################################\n",
        "\n",
        "\n",
        "# Loop over estimators to compare\n",
        "for n, (name, estimator) in enumerate(estimators.items()):\n",
        "    # Compute predictions\n",
        "    y_predict = np.zeros((1000, n_repeat))\n",
        "\n",
        "    for i in range(n_repeat):\n",
        "        estimator.fit(X_train[i], y_train[i])\n",
        "        y_predict[:, i] = estimator.predict(X_test)\n",
        "\n",
        "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
        "    y_error = np.zeros(1000)\n",
        "\n",
        "    for i in range(n_repeat):\n",
        "        for j in range(n_repeat):\n",
        "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
        "\n",
        "    y_error /= (n_repeat * n_repeat)\n",
        "\n",
        "    y_noise = np.var(y_test, axis=1)\n",
        "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
        "    y_var = np.var(y_predict, axis=1)\n",
        "    \n",
        "    print(\"{0}\\t: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
        "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
        "                                                      np.mean(y_error),\n",
        "                                                      np.mean(y_bias),\n",
        "                                                      np.mean(y_var),\n",
        "                                                      np.mean(y_noise)))\n"
      ],
      "id": "a7fd803b",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree\t: 0.0258 (error) = 0.0005 (bias^2)  + 0.0153 (var) + 0.0098 (noise)\n",
            "Bagging\t: 0.0190 (error) = 0.0007 (bias^2)  + 0.0082 (var) + 0.0098 (noise)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a65d1c1"
      },
      "source": [
        "Vous devriez constater que là où le gain d'erreur est le plus important est la variance. C'est exactement ce que nous avions préalablement anticipé."
      ],
      "id": "7a65d1c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gross-warner"
      },
      "source": [
        "## IV. Boosting\n",
        "\n",
        "L'idée première derrière la méthode de *boosting* est de construire des modèles \"faibles\" (potentiellement à peine meilleurs que le hasard) mais complémentaires entre eux possédant ainsi de bonnes performances en tant que groupe. Nous étudierons ici la méthode *AdaBoost* (i.e. Adaptive Boosting). Soit $\\mathcal{X}\\subseteq\\mathbb{R}^d$ et $\\mathcal{Y}=\\{-1, 1\\}$. On ne considèrera que le cas de la classification binaire bien que le propos se généralise à d'autres tâches de *machine learning*. Notre objectif est de construire un modèle final $h$ de $\\mathcal{X}$ vers $\\mathcal{Y}$ en s'appuyant sur un jeu de données $S_n$. Notons $w_i^j$ le poids associé à la donnée $(x_i, y_i)$ pour le modèle faible $j$. Initialisons les poids à $w_i^j=1/n$. Notre modèle consistera en $m$ classifieurs faibles.\n",
        "\n",
        "*AdaBoost* fonctionne de la manière suivante :\n",
        "\n",
        "1. On initialise notre compteur $j=1$ (i.e. on considère le premier modèle)\n",
        "2. On optimise notre modèle sur le jeu de données : $\\sum_i w_i^j\\textbf{1}\\{h_j(x_i)\\neq y_i\\}$\n",
        "\n",
        "3.  On calcule l'erreur normalisée : $\\epsilon_j=\\frac{\\sum_i w_i^j\\textbf{1}\\{h_j(x_i)\\neq y_i\\}}{\\sum_i w_i^j}$ et on évalue l'importance du modèle courant pour notre meta-modèle : $\\alpha_j=\\text{ln}\\Big(\\frac{1-\\epsilon_j}{\\epsilon_j}\\Big)$\n",
        "4.  On met à jour la pondération des données pour le modèle suivant : $w_i^{j+1}=w_i^j\\text{exp}\\big(\\alpha_j \\textbf{1}\\{h_j(x_i)\\neq y_i\\}\\big)$\n",
        "5.  Si $j<m$, on reprend à l'étape $2$ pour optimiser le modèle suivant.\n",
        "\n",
        "Une fois les modèles optimisés, notre *meta-classifieur* permet de faire des prédictions de la manière suivante :\n",
        "\n",
        "$$H(x)=\\text{sign}\\Big(\\sum_j\\alpha_j h_j(x)\\Big)$$\n",
        "\n",
        "Intuitivement, on cherche à favoriser les modèles qui \"fonctionnent bien\" et à donner de l'importance aux exemples d'apprentissage mal classés pour que les futurs classifieurs faibles se concentrent dessus."
      ],
      "id": "gross-warner"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "focused-madonna"
      },
      "source": [
        "----\n",
        "\n",
        "**Pourquoi ces coefficients ?**\n",
        "\n",
        "\n",
        "Considérons tout d'abord la *loss* exponentielle :\n",
        "\n",
        "$$\\ell(z)=\\text{exp}\\big(-z\\big),$$\n",
        "\n",
        "où $z=yh(x)$, $(x, y)\\in S_n$. La *loss* est affichée juste après. De manière directe, on observe que si notre modèle prédit un score dont le signe est le même que celui du label, alors la *loss* est petite et, à l'inverse, si le signe est différent, notre *loss* sera grande."
      ],
      "id": "focused-madonna"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "turkish-engineer"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "y = np.exp(-x)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x, y, label='Exponential loss')\n",
        "y = (x<=0).astype(int)\n",
        "x[51] = 0\n",
        "plt.plot(x, y, label='0/1 loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "turkish-engineer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "median-shannon"
      },
      "source": [
        "On cherche à construire un classifieur $H$ minimisant la *loss* exponentielle suivante :\n",
        "\n",
        "$$\\mathcal{L}(H)=\\sum_{i=1}^n\\ell(y_iH(x_i)),$$\n",
        "\n",
        "où \n",
        "\n",
        "$$H(x)=\\frac{1}{2}\\sum_j\\alpha_j h_j(x),$$\n",
        "\n",
        "tels que $h_j$ sont nos classifieurs faibles et $\\alpha_j$ les poids qui leur sont associés. Cependant, imaginons qu'au lieu de tout minimiser d'une seule fois, nous procédions itérativement, classifieur par classifieur. Ainsi, lorsqu'on optimise le classifieur $j$, tous les classifieurs $h_{1}, \\ldots, h_{j-1}$ et leur poid $\\alpha_1, \\ldots, \\alpha_{j-1}$ restent fixés. Notons $H_m(x)$ le classifieur total jusqu'au classifieur faible $h_m$. Notre *loss* se reformule ainsi :\n",
        "\n",
        "$$\\mathcal{L}_j(H)=\\sum_{i=1}^n\\text{exp}\\Big(-y_iH_{j-1}(x_i)-\\frac{1}{2}y_i\\alpha_jh_j(x_i)\\Big)=\\sum_{i=1}^nw_i^j\\text{exp}\\Big(-\\frac{1}{2}y_i\\alpha_jh_j(x_i)\\Big),$$\n",
        "\n",
        "où $w_i^j = \\text{exp}(-y_iH_{j-1}(x_i))$. Notons que si nous sommes bien entrain d'optimiser notre loss du point de vue de $h_j$ et $\\alpha_j$, alors $w_i^j, \\forall i$ sont des constantes. Notons $S_c^{m}$ l'ensemble des points correctement classés par $H_m$ et $S_i^m$ ceux qui à l'inverse ne le sont pas. Nous pouvons reformuler l'erreur précédente de la manière suivante :\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\mathcal{L}_j(H)&=e^{-\\alpha_j/2}\\sum_{i\\in S_\\mathcal{c}^j}w_i^m+e^{\\alpha_j/2}\\sum_{i\\in S_\\mathcal{i}^j}w_i^m\\\\\n",
        "&=(e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\}+e^{-\\alpha_j/2}\\sum_{i=1}^nw_i^m\\\\\n",
        "&\\propto (e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\}\n",
        "\\end{aligned}$$\n",
        "\n",
        "Du point de vu de l'optimisation de $h_j$, on remarque que cela revient à optimiser :\n",
        "\n",
        "$$(e^{\\alpha_j/2}-e^{-\\alpha_j/2})\\sum_{i=1}^nw_i^m\\textbf{1}\\{h_j(x_i)\\neq y_i\\},$$\n",
        "\n",
        "où le choix de $\\alpha_j$ n'a pas d'effet. Cela revient à réaliser l'étape $2$ de notre algorithme. Considérons maintenant l'optimisation de $\\alpha_j$. En annulant la derivée en fonction de $\\alpha_j$ on se rend compte que cela revient à le calculer comme indiqué à l'étape 3 ($\\epsilon_j$ et $\\alpha_j$).\n",
        "\n",
        "----\n",
        "\n",
        "Dans le cas de la régression, la stratégie va être de travailler sur les résidus des modèles précédents.\n",
        "\n",
        "---\n",
        "Un *stumps* est une fonction seuille. C'est une fonction qui prend une variable, et considère que toutes les données dont la valeur de cette variable est supériere (ou inférieure) à un seuil sont associées à la classe $1$ et les autres à la classe $-1$. Il s'agit tout simplement d'un arbre de profondeur $1$.\n",
        "\n"
      ],
      "id": "median-shannon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a5cb4e6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "id": "2a5cb4e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0f831ea"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**Dans le code ci-dessous, complétez les trois sections. La première consiste à construire un *stump*, à le fiter et à calculer son taux d'erreur sur le test. La seconde consiste à calculer un arbre de décision de profondeur $9$ et de nombre minimum de point par feuille à $1$, à le fitter et à calculer son taux d'erreur sur le test. Enfin, la troisième partie consiste à construire un modèle AdaBoost où le classifieur faible est un *stump*.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "b0f831ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7a41470"
      },
      "source": [
        "X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n",
        "\n",
        "X_test, y_test = X[2000:], y[2000:]\n",
        "X_train, y_train = X[:2000], y[:2000]\n",
        "\n",
        "####### Complete this part ######## or die ####################\n",
        "... stump instanciation\n",
        "... fit\n",
        "dt_stump_err = ...\n",
        "###############################################################\n",
        "\n",
        "####### Complete this part ######## or die ####################\n",
        "... decision tree instanciation\n",
        "... fit\n",
        "dt_err = ...\n",
        "###############################################################\n",
        "\n",
        "\n",
        "####### Complete this part ######## or die ####################\n",
        "n_estimators = 400\n",
        "learning_rate = 1.\n",
        "\n",
        "... AdaBoost instanciation\n",
        "... fit\n",
        "###############################################################\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n",
        "        label='Decision Stump Error')\n",
        "ax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n",
        "        label='Decision Tree Error')\n",
        "\n",
        "ada_err = np.zeros((n_estimators,))\n",
        "for i, y_pred in enumerate(ada.staged_predict(X_test)):\n",
        "    ada_err[i] = zero_one_loss(y_pred, y_test)\n",
        "\n",
        "ada_err_train = np.zeros((n_estimators,))\n",
        "for i, y_pred in enumerate(ada.staged_predict(X_train)):\n",
        "    ada_err_train[i] = zero_one_loss(y_pred, y_train)\n",
        "\n",
        "ax.plot(np.arange(n_estimators) + 1, ada_err,\n",
        "        label='AdaBoost Test Error',\n",
        "        color='red')\n",
        "ax.plot(np.arange(n_estimators) + 1, ada_err_train,\n",
        "        label='AdaBoost Train Error',\n",
        "        color='blue')\n",
        "\n",
        "ax.set_ylim((0.0, 0.5))\n",
        "ax.set_xlabel('Nombre d\\'estimateurs')\n",
        "ax.set_ylabel('Taux d\\'erreur')\n",
        "\n",
        "leg = ax.legend(loc='upper right', fancybox=True)\n",
        "leg.get_frame().set_alpha(0.7)\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "c7a41470",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raising-martin"
      },
      "source": [
        "## V. Les forêts aléatoires"
      ],
      "id": "raising-martin"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf9be48"
      },
      "source": [
        "Vous avez vu différentes manières d'agréger plusieurs classifieurs. Vous avez également étudié les arbres de décision. Un arbre pris individuellement, sans limite de profondeur possède une dimension VC infinie. Ou, dit autrement, il possède une forte capacité à sur-apprendre. Les *forêts aléatoires* ou *random forest* définissent une stratégie permettant d'agréger des arbres de décisions complémentaires. Chacun des arbres ne peut pas avoir été appris de la même manière puisque l'apprentissage est déterministe (on aurait ainsi $M$ arbres identiques et ça n'apporterait rien).\n",
        "\n",
        "Il existe de multiples stratégies permettant de construire des arbres différents et nous en détaillons une ici. Il existe deux angles d'attaque que nous combinerons :\n",
        "* Chaque arbre ne voit pas les mêmes données,\n",
        "* Chaque arbre ne voit pas les mêmes *features*.\n",
        "\n",
        "Une stratégie d'apprentissage est donc la suivante pour un des arbres :\n",
        "\n",
        "1.  On construit un jeu de données $S^\\prime$ en tirant uniformément des points de $S$ avec remise. Une fois que $|S^\\prime|=n^\\prime$, on s'arrête,\n",
        "2. En supposant que nous ayons $d$ variables explicatives, on tire uniformément $d^\\prime$ variables dans $d$,\n",
        "3. On construit l'arbre avec le jeu de données $S^\\prime$ et les $d^\\prime$ variables sélectionnées.\n",
        "\n",
        "Cette opération est répétée jusqu'à ce que tous nos arbres aient été construits. Ainsi en voyant moins de *features*, nos arbres sont plus stables et leur combinaisons ayant vu des informations différentes est elle-même plus stable."
      ],
      "id": "6cf9be48"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28dda641"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "id": "28dda641",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ee1be7"
      },
      "source": [
        "**<span style='color:blue'> Exercice</span>** \n",
        "**En récupérant les résultats déjà calculés dans Boosting pour *stump* et *decision tree*, complétez le code suivant afin de construire et de fitter un *random forest* tel que la profondeur maximale de l'arbre soit $9$.**\n",
        "\n",
        "\n",
        "\n",
        " ----"
      ],
      "id": "d7ee1be7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78b455aa"
      },
      "source": [
        "X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n",
        "\n",
        "X_test, y_test = X[2000:], y[2000:]\n",
        "X_train, y_train = X[:2000], y[:2000]\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 1.\n",
        "\n",
        "max_n_estimators = 50\n",
        "\n",
        "n_estimators_list = np.array(\n",
        "    list(range(1, max_n_estimators + 1))\n",
        ")\n",
        "\n",
        "rf_err = np.zeros((n_estimators_list.shape[0],))\n",
        "rf_err_train = np.zeros((n_estimators_list.shape[0],))\n",
        "\n",
        "for j, nb_estimators in enumerate(n_estimators_list):\n",
        "    ####### Complete this part ######## or die ####################\n",
        "    ... random forest construction\n",
        "    ... fit\n",
        "    ###############################################################\n",
        "    rf_err[j] = zero_one_loss(rf.predict(X_test), y_test)\n",
        "    rf_err_train[j] = zero_one_loss(rf.predict(X_train), y_train)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n",
        "        label='Decision Stump Error')\n",
        "ax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n",
        "        label='Decision Tree Error')\n",
        "\n",
        "ax.plot(n_estimators_list, rf_err,\n",
        "        label='RandomForest Test Error',\n",
        "        color='red')\n",
        "\n",
        "ax.plot(n_estimators_list, rf_err_train,\n",
        "        label='RandomForest Train Error',\n",
        "        color='blue')\n",
        "\n",
        "ax.set_ylim((0.0, 0.5))\n",
        "ax.set_xlabel('Nombre d\\'estimateurs')\n",
        "ax.set_ylabel('Taux d\\'erreur')\n",
        "\n",
        "leg = ax.legend(loc='upper right', fancybox=True)\n",
        "leg.get_frame().set_alpha(0.7)\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "78b455aa",
      "execution_count": null,
      "outputs": []
    }
  ]
}